############

kubernetes Core Concepts:

1) ETCD - Key-value distributed data store (check explore_etcd file for more information)

2) Kube-api server: First entry to the cluster using kubectl command, kubectl command authenicate first against ETCD DB and repsone back to the admin.
    - To view kube-api server in kubeadm setup, it deploys the api-server as a pod in kube-system namespace and default file under /etc/kubernetes/manifests/kube-apiserver.yaml
    -If deploy as a dedicated service, then we can explore more options under /etc/systemd/system/kube-apiserver.service

3) Scheduler: continuously monitor the API server and get the information if any new resouces needs to be created then if find out the right node for that resource and pass the information to API servers, then API server update that information to ETCD DB and pass the information to kubelet service to that particular node.
    - To view kube Scheduler in kubeadm setup, it deploys the Scheduler as a pod in kube-system namespace and default file under /etc/kubernetes/manifests/kube-scheduler.yaml
    -If deploy as a dedicated service, then we can explore more options under /etc/systemd/system/kube-scheduler.service

4) Controller Manager: continuously monitor the entire cluster and make sure to keep the cluster into desired state while functioning.
    - Node Controller: assures all nodes are up and running with healthy state. Using API-Server, keep the track of nodes in every 5 seconds and if its not responding then afte 40 seconds, it marked as unreachable then finally 5 mintues window started to keep waiting for a node to respond back and if not then afte 5 mintues, pods running on that nodes will be evicted.
    - Replica Controller: monitor the status of replicas sets and keep the same number of pods running which is desired. It maintains the load balanace and scale of pods. New name of this is Replica Set and in modern K8s, it is name as Replica Set.
    - Deployment Controller:
    - Namespcae Controller:
    - Enpoint Controller:
    - Job Controller:
    - Service Account Controller:
    and many more.

    - To view kube Controller manaer in kubeadm setup, it deploys the manager as a pod in kube-system namespace and default file under /etc/kubernetes/manifests/kube-controller-manager.yaml
    -If deploy as a dedicated service, then we can explore more options under /etc/systemd/system/kube-controller-manager.service


5) Kubelet: It register the node with kubernetes cluster and it receives a instruction from API server to deploy the resoruce and then instruct further to runtime engine to deploy the container and report back to API server.

6) Kube-proxy: Within cluster each pods or resrouce can talk to each other, no matter on which nodes they are running, this is possible becuase of network which expanded across all cluster. A pod can be exposed as a service and that service management done by kube-proxy which creates the forward rule and routes to backend assigned pod to that service. It can be done by IPTables.
It is deployed by kubeadm as Deamon Set (single pod) on each node.


** If we want to deploy these componants as a dedicated then we can download the binary from here, extract it, and then run it as a service. To download the kubecontroller-manager, we can use this url: https://storage-googleapis.com/kubernetes-release/release/v1.**/bin/linux/amd54/kubecontroller-Manager (replace the last part if you want to download other resources)


7) POD: Is a wrapper to a container, it can run one or more container at a time.

8) ReplicaSet: Maintain the number of of desired pods.
    # kubectl create -f <replica-file>.yaml
    Scaling the replicaset:
        1) update the yaml file with new numbers of replicas required and replace it with:
            # kubectl replace -f <replica-file>.yaml
        2) we can directly scale the replica from command line but that will not udpate the replica-file.yaml:
            # kubectl scale --replicas=<new_number> -f <replica-file>.yaml
            OR 
            # kubectl scale --replica=<new_number> replicaset <name_of_replicaset>
    NOTE ---> Replicaset can manage the all pods which has label as app: nginx-app whether or not these pods are created before the replicaset created.  

9) Deployment: It is a feature to upgrade the underlying instances or pods in a controlled manner. By default, it uses Rolling Updates to upgade the applicaiton or pods running where we can control the updates, rolled back, pause or resume the changes update.
Deployement is the top most layer in hierarchy, staring from container --> pod --> replica set --> deployment.
We can use the same yaml file which we have for replicaset (check understand_yaml.txt file) except the kind which will be deployment in this case. Once we create the deployment, it will create the replicaset and then pods.

10) Services: Which enables the communication outside of cluster for the applicaiton running inside the pods or inside as well with other PODs inside the cluster.
Service listen to a port on the Node and forward requests on that port to a port on the POD running the application.
    Type of Service:
        a) NodePort: it listen to a port on the Node and forward requests to pods using service port.
            - port range: 30000 to 32767
            - any port assigned to NODE IP from this range and then request will be forwarded to service port (which referes as PORT) and then service furhter forwarded the request to pod port (which referes to TargetPort) where actual application is running.

        b) ClusterIP: It creates a virtual IP inside the cluster to enable communication between different services such as front end and backend servers.
            It is a default type of service.

        c) LoabBalancer: It supported in cloud provider, to distribute the loads amoung all pods where applicaiton is running.
                ** For virtual environment, we can create a dedicated VM as LoabBalancer and set up it using nginx or HAProxy.

11) Namespcae:
        A isolated place for all its resources.
        We can create a dedicated or purpose wise namespace to work into it differently.
        Each namespace has their own resources, policies and roles.
        Within a namespace, resource can talk to each other with name only.
        But if any reosurce from one namespace need to connect with other resource in other namespace then it must use the full name of that resouce with namespace.
        e.g.
        <resouce_name>.<Namespace_name>.svc.cluster.local (where cluster.local is the default domain name of cluster)

        When a service is created, dns entry will be added in this format for each namespace.

        When we create any resource or list any resource, it will list the resources from default namespace or whereever you are working. If you want to list or create in other namespace, sitting in current namespace then you need to specify the --namespace or -n flag with command.

        In definition file also, we can specify the namespace under metadata for that resource. like:

        apiVersion: v1
        kind: Pod 
        metadata:
            name: my_pod
            namespace: dev
            labels:
                app: my_app
        spec:
            containers:
            - name: my_pod
              image: nginx

        --> to create a name space,
        # kubectl create namespace dev
        OR
        # using definition file:
        apiVersion: v1
        kind: Namespace 
        metadata:
            name: dev
        # kubectl create -f definition_file.yaml

        --> To switch to different namespace
        # kubectl config set-context $(kubectl config current-context) --namespace=dev
        ( this command will switch to dev namespace and set permanantly dev as current namespace)

        --> to list the all the pods or resources from all namespaces
        # kubectl get pods --all-namespace

        --> We can create or set the quota limit to a namespace
        we can define in definition file where kind is ResourceQuota

        apiVersion: v1
        kind: ResourceQuota
        metadata:
            name: compute-limit
            namespace: dev
        spec:
            hard:
                pods: "5"
                requests.cpu: "4"
                requests.memory: 5Gi
                limits.cpu: "10"
                limits.memory: 10Gi

        ** We can test the quota limit, by simply creating more containers
        # kubectl run web1 --image=amitaryan0010/path4cloud-web

        **>> Doing it as declarative approach with kubectl apply command

        while doing with apply, it creates a JSON file as well to compare the local configuration file with live configuration file. If no JSON file, then apply will consider a new object and create a live configurationfor that object. If object is there, then a local configuration will be compared with local and live and accordinlgly, decision will be make and perform the operation.

        JSON format data is also stored in live config file only under object metadata as annotations.

        metadata:
            annotations:
                kubectl.kubernetes.io/last-applied-configuration: |
                {"apiVersion":"v1","kind":"Namespace","metadata":{"annotations":{},"name":"dev"}}
                creationTimestamp: "2025-09-08T20:59:20Z"



12)  Scheduling:
        When we schedule any pode, by default scheduler will assign the node to a pods and adds another property in pod definition file in etcd as nodeName.

        # kubectl get pods web1 -o yaml  | grep -i node
        nodeName: k8node1

        Once this nodeName is assgined by creating the Binding Object then pod will be scheduled.

        If we want to assign a new pod to a particular node then we can set this property in pod definition file under spec section of container.

        And if we want to set this property for a existing pod then we can do by following only, not by editing the pod definition file.

        - we can create a manual binding object and send a post reqeust to the pod binding API. In the binding object you can specify the target node with the name of node. Then send a post request to the pods binding API with the data  set to the binding object in a JSON format. 
        we must convert the yaml format to json format for API call:

        apiVersion: v1
        kind: Binding
        metadata:
            name: <pod_name>
        target:
            apiVersion: v1
            kind: Node
            name: <node_name>

        YAML ---> JSON

        # curl --header "Context-Type:applicaiton/json" --request POST --data '{"apiVersion":"v1". "kind": "Binding", "metadata: "name":"<pod_name>"", "target".........} http://$SERVER/api/v1/namespaces/default/pods/$PODNAME/binding/

    --> Labels & Selectors:
        Lables: are just a tag or category which we can put for any object under metadata sections.

        Selectors: are the filter to select the right object using Labels which we put on object.

        Apart from the Labels and Selectors, we have annotations as well which might be defined for other purposes, like build information, chnange and release version for deployment, contact details for support or any other kind of integration.

        ** to filter or list the pods or any other object, we can use --selector flag:
        # kubectl get pods --selector key1=value1,key2=value2...
        (use --no-headers if you don't want to print the header, like NAME READY STATUS)


    --> Taints and Tolerations
        Taints: are a kind of restrcition or limitation which we put on Nodes, in order to allow only the choosen workload should be scheduled on tainted nodes. Let say, node1 is having a SSD or some special type of CPU/GPU and we want only compatiable or high performance applicaiton pods will be scheduled on this node, then we can put the taint on this node and then it will accept the only those pods which can handle the taint via tolerations.

        Tolerations: As specified above, which can handle the defined taint on nodes, only those pods will be scheduled on tainted nodes. So, Tolerations can be applied to PODS only.

        Let say, we have 3 worker node cluster and 3 pods, needs to be scheduled. We have applied the taint to Node1 and pod2 can tolerate that taint. Then, pod1 and pod3 can't be scheduled on Node1 whereas pod2 can be scheduled on Node1 but there is no gurantee it will scheduled on Node1, it can be scheduled on another node as well, until Affinity rules are not there.

        How to apply the taint and tolerations:

        Taint is just a key"value pair to apply the taint.

        # kubectl taint nodes <node_nam> key=value:taint:taint-effect

        ** we have 3 kinds of taint-effect:
        a) NoSchedule (no pods will be scheduled)
        b) PreferNoSchedule (system will try to avoid the schedule any pods)
        c) NoExecute (No new pods will be scheduled, and existing pods will be evicted if they are not able to tolerate)

        e.g.
        # kubectl taint nodes k8node1 disk=ssd:NoSchedule

        - to apply the tolerations on pod, we can add tolerations: property with other key:values as array under spec section in pod definition file:

    spec:
        tolerations:
        - key: "disk"
          operator: "Equal"
          value: "ssd"
          effect: "NoSchedule"

        ** Master node in kubernetes cluster is set as NoSchedule by default, so no any pods will be scheduled on master nodes except the kube-systems pods.

        # kubectl describe node k8master | grep -i taint
        Taints:             node-role.kubernetes.io/control-plane:NoSchedule

        ** To remove the taint from a node, get the taint effect and run this:
        # kubectl taint nodes <node_name> <put_taint_effect>- (make sure at the end, we put - (minus) symbol to remove this)

        Follow the documents for more information:
        https://kubernetes.io/docs/concepts/scheduling-eviction/taint-and-toleration/
        
    --> Node Selectors:
        There are 2 ways to define the pods to be scheduled on a particular node.
        1) with labels: we can set the labels to a node like any key:value pair and while defining the pod definition file, we can use the nodeSelector property and set that key:value pair and secheduler will match the lable and put the pods on desired node.
        * label a node first then define the same in pod definition file:
        # kubectl lable nodes <node_name> <key>:<value>
        # kubectl get nodes --show-labels

        * put the selector in file now under spec section:
        nodeSelector:
            <key>: <value>
        
        ** but we have limitation with nodeSelector if we need to put more complex filter like, if we have two lables keys or some condition based expression (OR, Not) then we can't acheive this with nodeSelector. So, let see 2nd option.

        2) Node Affinity:
        It also servers the same purpose but with advanced option. We need to define the affinity property in pod definition file.
        * setting in pod definition file:
        affinity:                                                   (property)
            nodeAffinity:                                           (in this case, its nodeAffinity)
                requiredDuringSchedulingIgnoredDuringExecution:     (Affinity Type)
                    nodeSelectorTerms:                              (it is an array wherer we specify the expression)
                    - matchExpressions:                             (this array will have key, operator and values)
                      - key: <label_key>                            (key will look for the key applied on nodes)
                        operator: In                                (operators is IN, which will make sure, any values from list)
                        values:                                     (list of values)
                        - <label_value_1>
                        - <label_value_2>
                           
                ** If you want to use NotIn Operator which will make sure, pod can be placed on any node which don't have below value.
                ** If you have a fixed key:value set to nodes, then we can simply ignore the values and set the operatore to "Exists" which will check only if the label_key is exits or not without checking its values.

                ** check the documents for more operator and nodeAffinity properties:
                https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/#node-affinity
                https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/#operators
 
        We need to read the documents and understand all the possible scenario to place the pods as per node affinity rules.
        Lets take a look to node affinity types:
        - Type of node affinity defines the behaviour of the scheduler with respect to node affinity and the stages in the lifecycle of the pod.
        - there are 2 types of node affinity:
        a) requiredDuringSchedulingIgnoredDuringExecution (with this type, it will check the affinity rule and if satisfies then only it will schedule, else not, with Ignored execution is if Lables are removed or updated, then that will not affect the curretly running pods)
        b) preferredDuringSchedulingIgnoredDuringExecution (with this type, it will check the affinity rule and even it doesn't satisfy then it will try to best node for scheduling, with Ignored execution is same as above)

    ** We can put Taint & Tolerations and nodeAffinity together to achive the more desired and controlled state.

    --> Resource limits 
        In any cluser or nodes, we have 3 main compute which is likely to be reqruired to run an applicaiton and those are CPU, Memory and Disk.
        Same way, in kubernetes also, to host a pods we need these compute resources and scheduler will check the node resources availability and check if a pod can accomodate to a particular node or find a good node where a pod can be scheduled.

        By default, when a pod is scheduled, kubernetes assumes a pod or container requrires a .5 cpu and 256 MiB memory which is a minimum requests from a pod to schedule and scheduler finds a node where it can be scheduled. But we can control this limit if we need more or less resources for an applicaiton to run. We can specify it while creating a container with kubectl command or in a resource definition file under spec section with resoruces property.

        (.5 is 500 m or milli for CPU, we can specify as much as low as 1m (or 0.01) for a CPU but not lower than that. If we specify 1 cpu then it means, a 1 phyical cpu or 1 core or 1 Hyperthread.)

        (for memory, we can specify 1M (1 megabytes = 1,000,000 bytes) or 1Mi (1 Mebibytes = 1,048,576 bytes))

        ** By default, kubernetes sets a limit of 1 vCPU and 512Mi of memory if a container requires a more CPU or Memory then it con go up to this limit.
        ** But we can set the limit to a container. Request is soemthing when a pod is started it can reqeust the specified amount of CPU and memory in definiion file and during its lifecycle, if its required more cpu or memory then it can go upto defined Limit for vCPU and Memory. For CPU, it can't go beyond defined limit since kubernetes throttled the CPU but for memory it can go but it will be terminated if it tries to use it constantly.
        ** it can be specified under resources section in pod definiion file.
        resources:
          requests:
            memory: "500Mi"
            cpu: 1
          limits:
            memory: "1Gi"
            cpu: 2
        (note: we need to set the resource for each container)

        ** if you want your pod to pick the default reqeust value then we need to define kind of LimitRange:
        apiVersion: v1
        kind: LimitRange
        metadata:
            name: memory-range
        spec:
            limits:
            - default:
                memory: 512Mi
              defaultRequest:
                memory: 256Mi
              type: Container

        ** Similarly, we can set the default CPU limit with kind LimitRange
        apiVersion: v1
        kind: LimitRange
        metadata:
            name: cpu-range
        spec:
            limits:
            - default:
                cpu: 1
              defaultRequest:
                cpu: .5
            type: Container

        ** https://kubernetes.io/docs/tasks/administer-cluster/manage-resources/memory-default-namespace/
        ** https://kubernetes.io/docs/tasks/administer-cluster/manage-resources/cpu-default-namespace/
        ** https://kubernetes.io/docs/tasks/configure-pod-container/assign-memory-resource




        















