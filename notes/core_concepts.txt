############

kubernetes Core Concepts:

1) ETCD - Key-value distributed data store (check explore_etcd file for more information)

2) Kube-api server: First entry to the cluster using kubectl command, kubectl command authenicate first against ETCD DB and repsone back to the admin.
    - To view kube-api server in kubeadm setup, it deploys the api-server as a pod in kube-system namespace and default file under /etc/kubernetes/manifests/kube-apiserver.yaml
    -If deploy as a dedicated service, then we can explore more options under /etc/systemd/system/kube-apiserver.service

3) Scheduler: continuously monitor the API server and get the information if any new resouces needs to be created then if find out the right node for that resource and pass the information to API servers, then API server update that information to ETCD DB and pass the information to kubelet service to that particular node.
    - To view kube Scheduler in kubeadm setup, it deploys the Scheduler as a pod in kube-system namespace and default file under /etc/kubernetes/manifests/kube-scheduler.yaml
    -If deploy as a dedicated service, then we can explore more options under /etc/systemd/system/kube-scheduler.service

4) Controller Manager: continuously monitor the entire cluster and make sure to keep the cluster into desired state while functioning.
    - Node Controller: assures all nodes are up and running with healthy state. Using API-Server, keep the track of nodes in every 5 seconds and if its not responding then afte 40 seconds, it marked as unreachable then finally 5 mintues window started to keep waiting for a node to respond back and if not then afte 5 mintues, pods running on that nodes will be evicted.
    - Replica Controller: monitor the status of replicas sets and keep the same number of pods running which is desired. It maintains the load balanace and scale of pods. New name of this is Replica Set and in modern K8s, it is name as Replica Set.
    - Deployment Controller:
    - Namespcae Controller:
    - Enpoint Controller:
    - Job Controller:
    - Service Account Controller:
    and many more.

    - To view kube Controller manaer in kubeadm setup, it deploys the manager as a pod in kube-system namespace and default file under /etc/kubernetes/manifests/kube-controller-manager.yaml
    -If deploy as a dedicated service, then we can explore more options under /etc/systemd/system/kube-controller-manager.service


5) Kubelet: It register the node with kubernetes cluster and it receives a instruction from API server to deploy the resoruce and then instruct further to runtime engine to deploy the container and report back to API server.

6) Kube-proxy: Within cluster each pods or resrouce can talk to each other, no matter on which nodes they are running, this is possible becuase of network which expanded across all cluster. A pod can be exposed as a service and that service management done by kube-proxy which creates the forward rule and routes to backend assigned pod to that service. It can be done by IPTables.
It is deployed by kubeadm as Deamon Set (single pod) on each node.


** If we want to deploy these componants as a dedicated then we can download the binary from here, extract it, and then run it as a service. To download the kubecontroller-manager, we can use this url: https://storage-googleapis.com/kubernetes-release/release/v1.**/bin/linux/amd54/kubecontroller-Manager (replace the last part if you want to download other resources)


7) POD: Is a wrapper to a container, it can run one or more container at a time.

8) ReplicaSet: Maintain the number of of desired pods.
    # kubectl create -f <replica-file>.yaml
    Scaling the replicaset:
        1) update the yaml file with new numbers of replicas required and replace it with:
            # kubectl replace -f <replica-file>.yaml
        2) we can directly scale the replica from command line but that will not udpate the replica-file.yaml:
            # kubectl scale --replicas=<new_number> -f <replica-file>.yaml
            OR 
            # kubectl scale --replica=<new_number> replicaset <name_of_replicaset>
    NOTE ---> Replicaset can manage the all pods which has label as app: nginx-app whether or not these pods are created before the replicaset created.  

9) Deployment: It is a feature to upgrade the underlying instances or pods in a controlled manner. By default, it uses Rolling Updates to upgade the applicaiton or pods running where we can control the updates, rolled back, pause or resume the changes update.
Deployement is the top most layer in hierarchy, staring from container --> pod --> replica set --> deployment.
We can use the same yaml file which we have for replicaset (check understand_yaml.txt file) except the kind which will be deployment in this case. Once we create the deployment, it will create the replicaset and then pods.

10) Services: Which enables the communication outside of cluster for the applicaiton running inside the pods or inside as well with other PODs inside the cluster.
Service listen to a port on the Node and forward requests on that port to a port on the POD running the application.
    Type of Service:
        a) NodePort: it listen to a port on the Node and forward requests to pods using service port.
            - port range: 30000 to 32767
            - any port assigned to NODE IP from this range and then request will be forwarded to service port (which referes as PORT) and then service furhter forwarded the request to pod port (which referes to TargetPort) where actual application is running.

        b) ClusterIP: It creates a virtual IP inside the cluster to enable communication between different services such as front end and backend servers.
            It is a default type of service.

        c) LoabBalancer: It supported in cloud provider, to distribute the loads amoung all pods where applicaiton is running.
                ** For virtual environment, we can create a dedicated VM as LoabBalancer and set up it using nginx or HAProxy.

11) Namespcae:
        A isolated place for all its resources.
        We can create a dedicated or purpose wise namespace to work into it differently.
        Each namespace has their own resources, policies and roles.
        Within a namespace, resource can talk to each other with name only.
        But if any reosurce from one namespace need to connect with other resource in other namespace then it must use the full name of that resouce with namespace.
        e.g.
        <resouce_name>.<Namespace_name>.svc.cluster.local (where cluster.local is the default domain name of cluster)

        When a service is created, dns entry will be added in this format for each namespace.

        When we create any resource or list any resource, it will list the resources from default namespace or whereever you are working. If you want to list or create in other namespace, sitting in current namespace then you need to specify the --namespace or -n flag with command.

        In definition file also, we can specify the namespace under metadata for that resource. like:

        apiVersion: v1
        kind: Pod 
        metadata:
            name: my_pod
            namespace: dev
            labels:
                app: my_app
        spec:
            containers:
            - name: my_pod
              image: nginx

        --> to create a name space,
        # kubectl create namespace dev
        OR
        # using definition file:
        apiVersion: v1
        kind: Namespace 
        metadata:
            name: dev
        # kubectl create -f definition_file.yaml

        --> To switch to different namespace
        # kubectl config set-context $(kubectl config current-context) --namespace=dev
        ( this command will switch to dev namespace and set permanantly dev as current namespace)

        --> to list the all the pods or resources from all namespaces
        # kubectl get pods --all-namespace

        --> We can create or set the quota limit to a namespace
        we can define in definition file where kind is ResourceQuota

        apiVersion: v1
        kind: ResourceQuota
        metadata:
            name: compute-limit
            namespace: dev
        spec:
            hard:
                pods: "5"
                requests.cpu: "4"
                requests.memory: 5Gi
                limits.cpu: "10"
                limits.memory: 10Gi

        ** We can test the quota limit, by simply creating more containers
        # kubectl run web1 --image=amitaryan0010/path4cloud-web

        **>> Doing it as declarative approach with kubectl apply command

        while doing with apply, it creates a JSON file as well to compare the local configuration file with live configuration file. If no JSON file, then apply will consider a new object and create a live configurationfor that object. If object is there, then a local configuration will be compared with local and live and accordinlgly, decision will be make and perform the operation.

        JSON format data is also stored in live config file only under object metadata as annotations.

        metadata:
            annotations:
                kubectl.kubernetes.io/last-applied-configuration: |
                {"apiVersion":"v1","kind":"Namespace","metadata":{"annotations":{},"name":"dev"}}
                creationTimestamp: "2025-09-08T20:59:20Z"



12)  Scheduling:
        When we schedule any pode, by default scheduler will assign the node to a pods and adds another property in pod definition file in etcd as nodeName.

        # kubectl get pods web1 -o yaml  | grep -i node
        nodeName: k8node1

        Once this nodeName is assgined by creating the Binding Object then pod will be scheduled.

        If we want to assign a new pod to a particular node then we can set this property in pod definition file under spec section of container.

        And if we want to set this property for a existing pod then we can do by following only, not by editing the pod definition file.

        - we can create a manual binding object and send a post reqeust to the pod binding API. In the binding object you can specify the target node with the name of node. Then send a post request to the pods binding API with the data  set to the binding object in a JSON format. 
        we must convert the yaml format to json format for API call:

        apiVersion: v1
        kind: Binding
        metadata:
            name: <pod_name>
        target:
            apiVersion: v1
            kind: Node
            name: <node_name>

        YAML ---> JSON

        # curl --header "Context-Type:applicaiton/json" --request POST --data '{"apiVersion":"v1". "kind": "Binding", "metadata: "name":"<pod_name>"", "target".........} http://$SERVER/api/v1/namespaces/default/pods/$PODNAME/binding/

    --> Labels & Selectors:
        Lables: are just a tag or category which we can put for any object under metadata sections.

        Selectors: are the filter to select the right object using Labels which we put on object.

        Apart from the Labels and Selectors, we have annotations as well which might be defined for other purposes, like build information, chnange and release version for deployment, contact details for support or any other kind of integration.

        ** to filter or list the pods or any other object, we can use --selector flag:
        # kubectl get pods --selector key1=value1,key2=value2...
        (use --no-headers if you don't want to print the header, like NAME READY STATUS)


    --> Taints and Tolerations
        Taints: are a kind of restrcition or limitation which we put on Nodes, in order to allow only the choosen workload should be scheduled on tainted nodes. Let say, node1 is having a SSD or some special type of CPU/GPU and we want only compatiable or high performance applicaiton pods will be scheduled on this node, then we can put the taint on this node and then it will accept the only those pods which can handle the taint via tolerations.

        Tolerations: As specified above, which can handle the defined taint on nodes, only those pods will be scheduled on tainted nodes. So, Tolerations can be applied to PODS only.

        Let say, we have 3 worker node cluster and 3 pods, needs to be scheduled. We have applied the taint to Node1 and pod2 can tolerate that taint. Then, pod1 and pod3 can't be scheduled on Node1 whereas pod2 can be scheduled on Node1 but there is no gurantee it will scheduled on Node1, it can be scheduled on another node as well, until Affinity rules are not there.

        How to apply the taint and tolerations:

        Taint is just a key"value pair to apply the taint.

        # kubectl taint nodes <node_nam> key=value:taint-effect

        ** we have 3 kinds of taint-effect:
        a) NoSchedule (no pods will be scheduled)
        b) PreferNoSchedule (system will try to avoid the schedule any pods)
        c) NoExecute (No new pods will be scheduled, and existing pods will be evicted if they are not able to tolerate)

        e.g.
        # kubectl taint nodes k8node1 disk=ssd:NoSchedule

        - to apply the tolerations on pod, we can add tolerations: property with other key:values as array under spec section in pod definition file:

    spec:
        tolerations:
        - key: "disk"
          operator: "Equal"
          value: "ssd"
          effect: "NoSchedule"

        ** Master node in kubernetes cluster is set as NoSchedule by default, so no any pods will be scheduled on master nodes except the kube-systems pods.

        # kubectl describe node k8master | grep -i taint
        Taints:             node-role.kubernetes.io/control-plane:NoSchedule

        ** To remove the taint from a node, get the taint effect and run this:
        # kubectl taint nodes <node_name> <put_taint_effect>- (make sure at the end, we put - (minus) symbol to remove this)

        Follow the documents for more information:
        https://kubernetes.io/docs/concepts/scheduling-eviction/taint-and-toleration/
        
    --> Node Selectors:
        There are 2 ways to define the pods to be scheduled on a particular node.
        1) with labels: we can set the labels to a node like any key:value pair and while defining the pod definition file, we can use the nodeSelector property and set that key:value pair and secheduler will match the lable and put the pods on desired node.
        * label a node first then define the same in pod definition file:
        # kubectl label nodes <node_name> <key>:<value>
        # kubectl get nodes --show-labels

        * put the selector in file now under spec section:
        nodeSelector:
            <key>: <value>
        
        ** but we have limitation with nodeSelector if we need to put more complex filter like, if we have two lables keys or some condition based expression (OR, Not) then we can't acheive this with nodeSelector. So, let see 2nd option.

        2) Node Affinity:
        It also servers the same purpose but with advanced option. We need to define the affinity property in pod definition file.
        * setting in pod definition file:
        affinity:                                                   (property)
            nodeAffinity:                                           (in this case, its nodeAffinity)
                requiredDuringSchedulingIgnoredDuringExecution:     (Affinity Type)
                    nodeSelectorTerms:                              (it is an array wherer we specify the expression)
                    - matchExpressions:                             (this array will have key, operator and values)
                      - key: <label_key>                            (key will look for the key applied on nodes)
                        operator: In                                (operators is IN, which will make sure, any values from list)
                        values:                                     (list of values)
                        - <label_value_1>
                        - <label_value_2>
                           
                ** If you want to use NotIn Operator which will make sure, pod can be placed on any node which don't have below value.
                ** If you have a fixed key:value set to nodes, then we can simply ignore the values and set the operatore to "Exists" which will check only if the label_key is exits or not without checking its values.

                ** check the documents for more operator and nodeAffinity properties:
                https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/#node-affinity
                https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/#operators
 
        We need to read the documents and understand all the possible scenario to place the pods as per node affinity rules.
        Lets take a look to node affinity types:
        - Type of node affinity defines the behaviour of the scheduler with respect to node affinity and the stages in the lifecycle of the pod.
        - there are 2 types of node affinity:
        a) requiredDuringSchedulingIgnoredDuringExecution (with this type, it will check the affinity rule and if satisfies then only it will schedule, else not, with Ignored execution is if Lables are removed or updated, then that will not affect the curretly running pods)
        b) preferredDuringSchedulingIgnoredDuringExecution (with this type, it will check the affinity rule and even it doesn't satisfy then it will try to best node for scheduling, with Ignored execution is same as above)

    ** We can put Taint & Tolerations and nodeAffinity together to achive the more desired and controlled state.

    --> Resource requests and limits 
        In any cluser or nodes, we have 3 main compute which is likely to be reqruired to run an applicaiton and those are CPU, Memory and Disk.
        Same way, in kubernetes also, to host a pods we need these compute resources and scheduler will check the node resources availability and check if a pod can accomodate to a particular node or find a good node where a pod can be scheduled.

        By default, when a pod is scheduled, kubernetes assumes a pod or container requrires a .5 cpu and 256 MiB memory which is a minimum requests from a pod to schedule and scheduler finds a node where it can be scheduled. But we can control this limit if we need more or less resources for an applicaiton to run. We can specify it while creating a container with kubectl command or in a resource definition file under spec section with resoruces property.

        (.5 is 500 m or milli for CPU, we can specify as much as low as 1m (or 0.01) for a CPU but not lower than that. If we specify 1 cpu then it means, a 1 phyical cpu or 1 core or 1 Hyperthread.)

        (for memory, we can specify 1M (1 megabytes = 1,000,000 bytes) or 1Mi (1 Mebibytes = 1,048,576 bytes))

        ** By default, kubernetes sets a limit of 1 vCPU and 512Mi of memory if a container requires a more CPU or Memory then it can go up to this limit.
        ** But we can set the limit to a container. Request is something when a pod is started it can reqeust the specified amount of CPU and memory in definition file and during its lifecycle, if its required more cpu or memory then it can go upto defined Limit for vCPU and Memory. For CPU, it can't go beyond defined limit since kubernetes throttled the CPU but for memory it can go but it will be terminated if it tries to use it constantly.
        ** it can be specified under resources section in pod definition file.
        resources:
          requests:
            memory: "500Mi"
            cpu: 1 
          limits:
            memory: "1Gi"
            cpu: 2
        (note: we need to set the resource for each container)

        ** if you want your pod to pick the default reqeust value then we need to define kind of LimitRange:
        apiVersion: v1
        kind: LimitRange
        metadata:
            name: memory-range
        spec:
            limits:
            - default:
                memory: 512Mi
              defaultRequest:
                memory: 256Mi
              type: Container

        ** Similarly, we can set the default CPU limit with kind LimitRange
        apiVersion: v1
        kind: LimitRange
        metadata:
            name: cpu-range
        spec:
            limits:
            - default:
                cpu: 1
              defaultRequest:
                cpu: .5
            type: Container

        ** https://kubernetes.io/docs/tasks/administer-cluster/manage-resources/memory-default-namespace/
        ** https://kubernetes.io/docs/tasks/administer-cluster/manage-resources/cpu-default-namespace/
        ** https://kubernetes.io/docs/tasks/configure-pod-container/assign-memory-resource


13) Daemon Sets:
    It is also a replica set which helps you to deploy the multiple replicas of pods across nodes but Daemonset will make sure, one copy of your pod must be running in each node. If a new node is added, automatically a replica of that pod is depoyed to new node and when the node is removed, obvisouly pod is also get removed.

    Use Case of Deamon Set:
        Monitoring Tools or agents
        Logs collector
        Kube proxy
        Networking Solution

    While defining it make sure, you put a Kind as DaemonSet 
    How scheduler will make sure, a pod as Daemonset will be scheduled on each node:
        until K8 v 1.12 - it uses the nodeName property and pu the node name in file and then scheduled the pod on each node.
        post that, it uses NodeAffinity with default scheduler.

14) Static Pods:
    These are standalone pods, not monitored by kube api server and can be deployed via definition file only. We have designated directory for Static Pods (check the /var/lib/kubelet/config.yaml) where the file can be placed and pods will get start automatically. 
    We can see, kudeadm uses the static pod mechanism to install the kubernetes, all definition files are can be find under /etc/kubernetes/manifests direcotry which is also a defualt direcotry for static path.
    check official information for more info:
    https://kubernetes.io/docs/concepts/workloads/controllers/daemonset/


15) Monitoring:
    It is a right hand of any IT personnal, without monitoring we can't live. We need to know what is happening with our cluster, in terms of nodes health, compute usage, applicaiton performance, network, disk usage, pod consumption.

    Solution to monitor the cluster componants:
    1) metric servers
    2) Prometheus
    3) Elastic Stacks
    4) Datadog
    5) Dynatrace

    Monitoring concepts came from HeapSter project which enabled the monitoring and analysis feature for kubernetes which is deprecated now and trimmed down to Metrics Server. Which run in kube-system namespace per cluster. Which retrives metrics from each nodes and pods and stores them in memory, so a result these are not persistent data. 
    In each node, we have kubelet running along with sub componant called cAdvisore (container advisor) which actually collects the performance from pods and send back to Metrics server using kublet-> API server.

    For minikube setup, we can add this as addon:
        # minikube addons enable metrics-server

    For full cluster, we can deploy this as standalon pods:
        # kubectl apply -f https://github.com/kubernetes-sigs/metrics-server/releases/latest/download/components.yaml
        OR
        # git clone https://github.com/kubernetes-incubator/metrics-server
        # kubectl create -f deploy/
        (it will deploy pods, services and roles to enable metrics servers to check the performance from the nodes of the cluser)
        root@k8master:~/metrics-server# kubectl top node
        NAME       CPU(cores)   CPU%   MEMORY(bytes)   MEMORY%
        k8master   354m         17%    1285Mi          68%
        k8node1    97m          4%     758Mi           40%

        root@k8master:~/metrics-server# kubectl top pods -A
        NAMESPACE     NAME                               CPU(cores)   MEMORY(bytes)
        default       nginx                              0m           13Mi
        default       web1                               0m           9Mi
        kube-system   coredns-55cb58b774-jsznq           6m           33Mi
        kube-system   coredns-55cb58b774-pfvng           5m           41Mi
        kube-system   etcd-k8master                      85m          80Mi

        OR for practices, we can clone from kodekloudhub as well:
        # git clone https://github.com/kodekloudhub/kubernetes-metrics-server.git

16) Logging:
    --> in docker
        # docker run kodekloud/event-simulator
        OR
        # docker -d kodekloud/event-simulator (to run as daemon)
        and then check the live logs:
        # docker logs -f <container_id>

    --> in kubernetes:
        - create a file to deploy this pod:
        apiVersion: v1
        kind: Pod
        metadata:
            name: event-simulator-pod
        spec:
            containers:
            - name: event-simulator
              image: kodekloudhub/event-simulator

        - then create the pod and run the Logs
        # kubectl create -f <file.yaml>
        # kubectl logs -f event-simulator-pod

17) Deployement Strategies:   
    We have different strategy to manage the lifecycle of an applicaiton in kubernetes:
    When we deploy the applicaiton as deploymnet then k8s will automatically creates the rollout with its versions. When we deploy a new version of same app that will mark as a new version and if anything is not good with new version then we can easily rollback to old version which was good. This is default behaviour for any strategy we follow, we have following availables:
    We can check the rollout history and status:
    # kubectl rollout status deployment <deployment_name>
    # kubectl rollout history deploymnet <deployment_name>
    To undo a change:
    # kubectl rollout undo deployment <deployment_name>

    a) Recreate:
        where we delete the existing pods with current image and then deploy the new pods with updated image, but we have downtime with this strategy. 
    b) Rolling updates:
        where a each pod will be replaced with a new pod with udpated image by defulat one by one, not all at a time, so no downtime will be occurred. And this is also a default strategy in kubernetes.

    Update can be anything, since we have a definition file for our deployment so when ever we have a rquirement, we can change in that file and apply it again. Then deloyment will create a new replica set and create the new pod.
    There are two ways:
        * after making changes in file, apply it.
            # kubectl apply -f file.yaml (this way we have track of a new updated file)
        * directly we can set the new parameter or property
            # kubectl set image deployment <deployment_name> (this way, it will not update your config file)

        * to check the status of rollout:
        # kubectl rollout history deploymnet <deployment_name>

        * if want to rollback
        # kubectl rollout undo deployment <deployment_name>

18) Additional field to container Spec Section:
        -  command and argument:
            If we need to pass the some command and entrypoint instructions in yaml file, then we can define as below:
            command: ["Binary"] (is equilant to entrypoint)
            args:["< >"] (is equilant to CMD)
            e.g:
            in spec section, under container section where image is mentioned, we can define this:
            command:["sleep"]
            args: ["10"]
    
            OR (we can pass as an array)
            command:
            - "sleep"
            - "10"
    
            OR
            command: ["sleep", "10"]

        - environment variables in yaml file:
            we can define this env property under spec container section:
            env:
                - name: < >
                  value: < >
    
            * env is an array which has its items as key:value pair, where name is the environment variable and value is value to that environment.

            ** we can define the environment values using configmaps and secret as well:
            env:
                - name: <env_name>
                  valueFrom:
                    configMapKeyRef:
                        name: <configmap_name>
                        key: <key_name>

            env:
                - name: < >
                  valueFrom:
                    secretKeyRef:
                        name: <secret_name>
                        key: <key_name>

19) ConfigMap:
        is used to pass the configuration data in the key value pair to the pod, like you need to pass the environment values or some meta data of app then we can inject with configmaps.
        Creating the configmaps via command line:
        -  Create a new config map with key:value pair
        # kubectl create configmap <configmap_name> --from-literal=<key>=<Value>

        - Create a new config map named my-config based on folder bar
        # kubectl create configmap my-config --from-file=path/to/bar

        - Using declarative approach with definition file:
        apiVersion: v1
        kind: ConfigMap
        metadata:
            name: my-config
        data:
            user: root
            psd: Cloud@123

        # kubectl create -f <file_name>

        ** injecting the configmap in pod definition file:
        under container in spec section, we can put the new property called "envFrom" and introduce our configmap:
        (envFrom is a list and can accept many Config in smae file)

        envFrom:
            - configMapRef:
                name: <configmap_name>

        ** It can be injected from file as well as Volumes property (will see later)
        ** but configmap is stored the value in clear text format

20) Secrets:
    is used to store the secret information like access keys or password since it stored the values in encrypted format.
        - Create a new secret with key:value pair
        # kubectl create secret generic <secret_name> --from-literal=<key>=<Value>

        - Using declarative approach with definition file:
        apiVersion: v1
        kind: ConfigMap
        metadata:
            name: my-secret
        data:
            user: root
            psd: Cloud@123

        ** But we need to store the values in hashed format.
        ** to convert the data into hashed format with base64 encoded format:
        # echo -n 'root' | base64
        # echo -n 'Cloud@123' | base 64

        and then put the generated hashed value into secret yaml file.
        ** to decode the value if required then:
        # echo -n 'hashed_value' | base64 --decode (will show the clear text value)

        ** injecting the secret in pod definition file:
        under container in spec section, we can put the new property called "envFrom" and introduce our secret:
        (envFrom is a list and can accept many secret in smae file)

        envFrom: 
            - secretRef:
                name: <secret_name>

        ** It can be injected from file as well as Volumes property (will see later)
        volumes:
        - name: <voluem_name>
          secret:
            secretName: <secret_name>

        ** to make the secret avaialble as files we need to create a file with key name and put the value inside the file.

21) Multicontainer Pods:
        When we need a functionality to work together like webserve rand logging agent that can scale up and down together.
        Each container in single pods shares the same network/storage and refer to each other as localhost

        Basically, when we design the multi container pod then we have 3 patters:
        a) Sidecar container (logging purpose)
        b) Adapter
        c) Ambassador


22) Init Container:
    When a requirement like a container can run only once (in order to run a task which is rqeuired only once like update or pulling data from repo), then we can inroduce the init container under container sectiom, this init container will run only when the pod is created very first time and after that if pod restarts then it will never run the init container, rest container will be started only and only when init container is succeded.

23) Cluster Maintenance:
        - OS upgrade
        - Implication of loosing a node from the cluster
        - How to take a node out of cluseter and add back
        - Cluster upgade

        - kubernetes releases and versions:

        a) Drain/Cordon/Uncordan:
            When we take the node out of cluster then we can drain the node or cordon it to mark it unschedulable for any new pods then we can perform the manitenance on this node.
            # kubectl drain <node_name> (drian operation moved all the workloads/pods to another node in cluster and mark the node as unschedulable)
            OR
            # kubectl cordon <node_name> (this operation will mark the node as unschedulable but will not move the pods to another nodes)

            ** drain can't delete or move the DaemonSet pods to different nodes.
            ** If any standlaone pod is running on target node then we have to do drain with --force option.

            ** once the node is drained or all pods are evicted then we can take it down and update the node.

        b) kuberenetes releases:
            # kubectl get nodes - shows the version like: v1.30.14
            here v1 - major version
                 30 - is minor version which usually released in every couple of months with new features and functionalities.
                 14 - is patch version to fix the bug

                 It has alpha and beta and then stable stages to make the fixes or new versions avaialble.

            with kuberenetes, as a part of same project other componant also gets the same version and pathes like:
                kube-apiserver
                controller-manager
                kube-scheduler 
                kubelet 
                kube-proxy
                kubectl

            but other componant may have different versions since they are part of different projects:
                ETCD Cluster
                CoreDNS

        c) Cluster Upgrade Progress:
            Though, we discuss the all componants of kuberenetes should have the same version but it can have different release and versions.
            Kube-apiserver is main componant in kuberenetes architecture so it must be having the same or higher version from another componant.
            e.g.
            Kube-apiserver                      ---> version N
            contoller-manager, kube scheduler   ---> version N-1 (can be one lower version than kube-apiserver or same versions)
            kubelet, kube-proxy                 ---> version N-2 (can be two lower version than kube-apiserver or same versions)

            whereas KUBECTL can have one higher version than kube-apiserver or same version or one lower version which allows us to bring up live updates.
            kubectl                             ---> N+1 OR N OR N-1

            ** we can also upgrade the componant to componant if required.

            ** At any time, kuberenetes supports only up to recent three minor versions. Let say, you are at v1.30 so it can support, v1.29 and v1.28 but not v1.27

            Master Node Upgrade:
                a) If in cloud, then few click and cluster will be upgraded.
                b) kubeadm approach (kubeadm upgrade plan, and then upgrade)
                c) Individual componant from scratch then manually upgrade the different componant.

            Steps to follow:
                a) Upgrade master node first and then worker nodes.
                b) while upgrading master node at that time api-server, scheduler and contorl manager is down so all management fucntion is down but worker nodes and already running pods still continue to run. We can't access the cluster or if any pods down then no healing, no new pods will be scheduled.
                c) now we can follow different strategy to upgrade the nodes:
                    i) all at a time (complete downtime)
                    ii) one node at a time, shift current workload from node to other nodes, upgrade it and join back.
                    iii) add a newly created node with latest version, shift workload from old node to new node until all nodes are gets replaced.

                # kubeadm upgrade plan <version_number> (will list all the current and target vesion)

                ** check cluster-upgrade-logs.txt under notes directory in this repository.

            ####################################
            ####################################

                CLUSTER Backup and Restore (check cluster-restore.txt under nodes direcotry in this repository)

            #####################################
            #####################################

            What we can backup?
                - Object Definition file: Keep at central place like Github to make the copy of all files.
                - Resource configuration: you can query the api server and store all the resources in a yaml file and take the back of that file.
                    # kubectl get all --all-namespaces -o yaml > all-resources.yaml
                    (we can use the tool like Velero by Haptio which will help you to take the backup of cluster using kube-apiserver.) 
                - ETCD Cluster: It stores the information about the state of our cluster, like nodes, cluster info, and all resources.
                    It has a preferred directory where it stores all the information, we can tkae the backup of that folder.
                    ETCD comes with inbuilt snapshot solution, which can be taken using etcdctl command like (# etcdctl snapshot save snapshot.db), once created then we can check the status using (# etcdctl snapshot status snapshot.db)
                    To restore the cluster from etcd backup: 
                    a) stop the kube-api sevrer (which will restrict the new resource added or restore all requrired to start it again.)
                    b) then run:
                        # etcdctl snapshot restore --data-dir <path_to_snapshot.db_file_directory_to_restore> <path_to_snapshot.db>
                    c) configure the etcd configuration amnifest file (and change the hostpath to new restored directory) or service file to use the new location.
                    d) systemctl daemon-reload
                    e) restart etcd server
                    f) restart the kube-api server

                    ** if required, we need to specify the etcd ca and certificate file for authenication, endpoints for etcd cluster and keys for etcd server)
                    (# etcdctl snapshot save --endpoint=<> --cacert=<> --cert=<> --key=<>)
                    (check explore_etcd.txt file)
                    root@k8master:~# kubectl exec -it etcd-k8master -n kube-system -- etcdctl version
                    etcdctl version: 3.5.15
                    API version: 3.5

                    root@k8master:~# kubectl exec -it etcd-k8master -n kube-system -- etcdctl snapshot save -h
                    NAME:
                            snapshot save - Stores an etcd node backend snapshot to a given file

                    USAGE:
                            etcdctl snapshot save <filename> [flags]

                    OPTIONS:
                    -h, --help[=false]    help for save

                    GLOBAL OPTIONS:
                        --cacert=""                               verify certificates of TLS-enabled secure servers using this CA bundle
                        --cert=""                                 identify secure client using this TLS certificate file
                        --command-timeout=5s                      timeout for short running command (excluding dial timeout)
                        --debug[=false]                           enable client-side debug logging
                        --dial-timeout=2s                         dial timeout for client connections
                    -d, --discovery-srv=""                        domain name to query for SRV records describing cluster endpoints
                        --discovery-srv-name=""                   service name to query when using DNS discovery
                        --endpoints=[127.0.0.1:2379]              gRPC endpoints
                        --hex[=false]                             print byte strings as hex encoded strings
                        --insecure-discovery[=true]               accept insecure SRV records describing cluster endpoints
                        --insecure-skip-tls-verify[=false]        skip server certificate verification (CAUTION: this option should be enabled only for testing purposes)
                        --insecure-transport[=true]               disable transport security for client connections
                        --keepalive-time=2s                       keepalive time for client connections
                        --keepalive-timeout=6s                    keepalive timeout for client connections
                        --key=""                                  identify secure client using this TLS key file
                        --password=""                             password for authentication (if this option is used, --user option shouldn't include password)
                        --user=""                                 username[:password] for authentication (prompt if password is not supplied)
                    -w, --write-out="simple"                      set the output format (fields, json, protobuf, simple, table)

                ** we can describe the pods for more information:

                etcd
                --advertise-client-urls=https://10.0.0.4:2379
                --cert-file=/etc/kubernetes/pki/etcd/server.crt
                --client-cert-auth=true
                --data-dir=/var/lib/etcd
                --experimental-initial-corrupt-check=true
                --experimental-watch-progress-notify-interval=5s
                --initial-advertise-peer-urls=https://10.0.0.4:2380
                --initial-cluster=k8master=https://10.0.0.4:2380
                --key-file=/etc/kubernetes/pki/etcd/server.key
                --listen-client-urls=https://127.0.0.1:2379,https://10.0.0.4:2379
                --listen-metrics-urls=http://127.0.0.1:2381
                --listen-peer-urls=https://10.0.0.4:2380
                --name=k8master
                --peer-cert-file=/etc/kubernetes/pki/etcd/peer.crt
                --peer-client-cert-auth=true
                --peer-key-file=/etc/kubernetes/pki/etcd/peer.key
                --peer-trusted-ca-file=/etc/kubernetes/pki/etcd/ca.crt
                --snapshot-count=10000
                --trusted-ca-file=/etc/kubernetes/pki/etcd/ca.crt


                - Persistent Volumes


24) Storage:
        Kubernets uses docker drives and volume plugins to manage the volumes as storage.
        Storage Driver: (used by container ir images to write the data)
            AUFS(default for Ubuntu server), ZFS, BTRFS, Device Mapper (RHEL), Overlay, Overlay2(docker)
        
        Volumes Plugins: (used to create the persistent volumes)
            Local(Default plugin), Azure File Storage, Convoy, Digital Ocean Block Storage, Floker, gce-docker, GlusterFS, NetApp, RexRay (AWS EBS), Portworx, VMware VSphere Storage

        Like, CRI (Container Runtime), CNI (Container Network interface), we have CSI which is Container Storage interface. CSI is a global standard which allows any container orchestration tool like kubernetes to work with any storage vendor with a supported plugins.

        Kubernetes, Cloud Foundary and Mesos are onboarded to CSI.

        CSI has there defined RPCs(remote procedure calls), which make sure calls must be done by orchestrator tools and must be implemented by the storage driver.
        RPC Rules: 
            CreateVolume
            DeleteVolume
            ControllerPublishVolume

       Volumes:
        attached to pods, and data generated by pods is not stored into volumes.
        we put the volumes in pod definition file, multiple ways to handle the volumes:
        a) for local directory from node (recommended for single node cluster) where pod is running, we put this as hostPath which is volumes plugins as below:
        at spec indentation, we put:
        volumes:
        - name: <voluem_name>
          hostPath:
            path: <path_to_local_dir>
            type: Directory

        at container indendation:
        volumeMounts:
        - mountPath: <dir_to_mount>
          name: <volume_name>
        ** it spportes others as well like AWS EBS, NFS, GlusterFS etc...

       Persistent Volumes (PV):
        Volume is being defined in pod definition file, for one or two pods its fine but what if we need to manage lot of pods then better to manage the volumes as persistent volumes which is a kind of managing the volumes centrally. PV is being created by administrator.
        PV is large amount of storage and useres or pod can claim the required volume from that and mount as PVC (persistent volume claims)
        creating a PV as definition file:
        apiVersion: v1
        kind: PersistentVolume
        metadata:
            name: <pv_name>
        spec:
            accessMode:                 #### this will define how the volumes will be mounted on target pod, we have ReadOnlyMany, ReadWriteOnce, ReadWriteMany
                - ReadWriteOnce
            capacity:
                storage: <size_of_pv>   #### 10Gi
            PersistentVolumeReclaimPolicy: <policy_name>   #### Retain/Delete/Recycle
            hostpath:                   #### its a volume type    
                path: <path_to_local_dir>
                type: Directory

        Persistent Volume Claims:
            To make the storage from PV Pool inside the pod, its being created by users.
            Kubernetes bind the PVC to PV based on reqeust. While binding, it checks for Sufficient capacity, Access modes, volumes modes, storage class and Selectors. It is one-to-one relationship between PV and PVC so remaining capacity in PV can't be used by other PVCs.
            creating a PVC as definition file:
            apiVersion: v1
            kind: PersistentVolumeClaim
            metadata:
                name: <pvc_name>
            spec:
                accessMode:
                    - ReadWriteOnce
                resources:
                    requestes:
                        storage: <requested_size>

            ** When you delete the PVC, PV will be deleted or not that depends on policy:
                - PersistentVolumeReclaimPolicy: Retain (with this policy, PV won't be deleted until admin will delete it but it is not avialable for reuse by any other claim)
                - PersistentVolumeReclaimPolicy: Delete (it will delete automatically after the PVC is deleted)
                - PersistentVolumeReclaimPolicy: Recycle (data will be deleted from volumes before it making it to avialble to other claims)

                (for more info refer, bonus_info_on_storage.txt file under cheatsheet dir)

            ** putting in pod definition file:
            at spec indentation, we put:
            volumes:
            - name: <volume_name>
              PersistentVolumeClaim:
                claimName: <pvc_name>
                
            at container indendation:
            volumeMounts:
            - mountPath: <dir_to_mount>
              name: <volume_name>  


        Storage Class:
            when we want the PVC from external storage then every time we need to create the disk first and then claim it but it would be great if disks gets created automatically in external storage when we can claim it with PVC, there Storage Class helps us, which is called a dynamic provisioning. So in order to acheive that, we need to create a storage provisioner which can automatically provision the storage and then claimn it with PVC, no need to create a PV in this case, its going to be created automatically.

            storage class definition file:
            apiVersion: storage.k8s.io/v1
            kind: StorageClass
            metadata:
                name: <sc_name>
            provisioner: <provisioner_name>

            and in PVC definition file:
            add new property called:
                storageClassName: <sc_name>

            ** while defining the provisioner in sc.yaml file, we can pass the additonal parameter as well like disk type, replicaiton, backup. Each provisioner has their specific parameter. For more info, check the documents:  https://kubernetes.io/docs/concepts/storage/storage-classes/

            https://kubernetes-csi.github.io/docs/drivers.html


25) Kubernetes security
    --> security Permitive:
            * secure the Hosts with only key based access
            * Secure Kube-apiserver access for administrator, service accounts or developers.
                - Authentication
                    token based
                    certificate based
                    LDAP
                - Authorization
                    RBAC
                    ABAC (Attribite based)
                    Node Authorization
                    Webhook Mode
            * All kubernetes componant communicates with each other on TLS based communication
            * By default, Pods can access all other pods, that can be secured using network policies.

        ** Naming convention:
            usually public key are names as .crt or .pem
            usually private key are names as .key or -key.pem (either as extention .key or in name itself -key)

    --> TLS based communication between all componant:
            We have many certifactes availability at /etc/kuberenetes/pki which are generated and singed by kubernetes at the time of installing.

            root@k8master:~# ls /etc/kubernetes/pki/
            apiserver-etcd-client.crt  apiserver-kubelet-client.crt  apiserver.crt  ca.crt  etcd                front-proxy-ca.key      front-proxy-client.key  sa.pub
            apiserver-etcd-client.key  apiserver-kubelet-client.key  apiserver.key  ca.key  front-proxy-ca.crt  front-proxy-client.crt  sa.key

            root@k8master:~# openssl x509 -in /etc/kubernetes/pki/ca.crt -noout -text
            Certificate:
                Data:
                    Version: 3 (0x2)
                    Serial Number: 6896756667732836252 (0x5fb633a61285bf9c)
                    Signature Algorithm: sha256WithRSAEncryption
                    Issuer: CN = kubernetes
                    Validity
                        Not Before: Sep 24 20:18:04 2025 GMT
                        Not After : Sep 22 20:23:04 2035 GMT

    --> We can generate our own TLS certifactes for kubernetes (we can use easyrsa or openssl to generate one)
            - generate keys         # openssl genrsa -out ca.key 2048
            - generate csr keys     # openssl req -new -key ca.key -subj "/CN=Path4Cloud-Cluster-CA" -out ca.csr
            - sign ceritifcate      # openssl x509 -req -in ca.csr -signkey ca.key -out ca.crt  

        Now we can generate other client certificates using this private key and root certifcate (ca.crt)

            For Admin user:
            - generate keys         # openssl genrsa -out admin.key 2048 
            - generate csr keys     # openssl req -new -key admin.key -subj "/CN=Cluster-Admin/O=system:masters" -out admin.csr
            - sign ceritifcate      # openssl x509 -req -in admin.csr -CAcert ca.crt -CAkey ca.key -out admin.crt    <--- now we sing all csr using our ca key and cert.

            For Kube Api Server, we must put the all alternative names becuase, any componant may refer to any names:
            - generate keys         # openssl genrsa -out kube-apiserver.key 2048 
            - generate csr keys     # openssl req -new -key kube-apiserver.key -subj "/CN=kube-apiserver" -out kube-apiserver.csr -config openssl.cnf
            - sign ceritifcate      # openssl x509 -req -in kube-apiserver.csr -CAcert ca.crt -CAkey ca.key -out kube-apiserver.crt -extentions req_ext -extfile openssl.cnf -days 365    
            (here since we need to specify many alternative names so that we can put all in openssl.cnf file and pass other information)
            # cat openssl.cnf
            [ req ]
            default_bits       = 2048
            prompt             = no
            default_md         = sha256
            req_extensions     = req_ext
            distinguished_name = dn

            [ dn ]
            C  = IN
            ST = HR
            L  = Gurgaon
            O  = Path4Cloud
            CN = kube-apiserver

            [ req_ext ]
            subjectAltName = @alt_names

            [ alt_names ]
            DNS.1   = kube-apiserver
            DNS.2   = kubernetes
            DNS.3   = kubernetes.default
            DNS.4   = kubernetes.default.svc
            DNS.5   = kubernetes.default.svc.cluster.local
            IP.1    = 10.0.0.4
            IP.2    = 127.0.0.1

            We can check this later on:
            root@k8master:/etc/kubernetes/pki# openssl x509 -in apiserver.crt -text -noout | grep -i -A1 "Alternative Name"
            X509v3 Subject Alternative Name:
                DNS:k8master, DNS:kubernetes, DNS:kubernetes.default, DNS:kubernetes.default.svc, DNS:kubernetes.default.svc.cluster.local, IP Address:10.96.0.1, IP Address:10.0.0.4

        and same way we can generate these certificates for other componand as well.
        We must generate some SOPs or sheet where we can maintain the all certificates details like, who issue, expiry data or CN name to check the health on regular basis.
        
    --> To access the api server:
            - when we run kubectl command, it uses the credentials from .kube/config file (generally under home directory of same user)
              That kubeconfig file contains:
                > Cluster API server address
                > CA certificate (ca.crt) to validate the API server
                > User credentials (usually a client certificate + key, e.g. client.crt / client.key, or a token)
            (/etc/kubernetes/admin.conf  contains certs for the kubernetes-admin user)

            - to access via curl it point to /etc/kubernetes/pki/apiserver-kubelet-*
                root@k8master:/etc/kubernetes/pki# curl https://10.0.0.4:6443/api/v1/namespaces/default/pods   --cert /etc/kubernetes/pki/apiserver-kubelet-client.crt   --key /etc/kubernetes/pki/apiserver-kubelet-client.key   --cacert /etc/kubernetes/pki/ca.crt
                {
                    "kind": "PodList",
                    "apiVersion": "v1",
                    "metadata": {
                        "resourceVersion": "205945"
                    },
                    "items": [

    --> If we want to manage multiple users CSR request then kubeadm master node is also acting as CA and we have certificateSigningRequest object in kubernetes which a admin can create and approve or reject using Certificates API.

        These all Certificate related tasks are carried out by Controller Manager (CSR-Approving, CSR-SIGNING)

        # kubectl get csr (will list all CSR request and its status)

        So the process is:
            step1) creates CSR request when we have csr file from user
            step2) review the request
            step3) approve (or reject)
            step4) share certs to users (if approved)

        demo:
            1) add a new user, set the password for that user and switch to same user:
            2) generate a key and csr request from that user and share the same in encoded 64 format with admin
                sam@k8master:~$ openssl genrsa -out sam.key 2048
                sam@k8master:~$ openssl req -new -key sam.key -subj "/CN=sam" -out sam.csr
                sam@k8master:~$ ls
                sam.csr  sam.key

                sam@k8master:~$ cat /home/sam/sam.csr | base64 | tr -d "\n" (use trim to get in one line or use -w 0)
                LS0tLS1CRUdJTiBDRVJUSUZJQ0FURSBSRVFVRVNULS0tLS0KTUlJQ1V6Q0NBVHNDQVFBd0RqRU1N
                QW9HQTFVRUF3d0RjMkZ0TUlJQklqQU5CZ2txaGtpRzl3MEJBUUVGQUFPQwpBUThBTUlJQkNnS0NB
                UUVBdHdJOE5NaGxibFJRQm9XMVVUbGRMUkNNUm1HNWZPSlpDZ1Fjcy9GOERmUllBL2t0CnZPQk1s
                Nk1rRjFDQWRsV1NieW5FWnZWUVo5TFVkYkwwbTdsVkRLM2oxNFloY04xVVhZTEMzWnBLZURPaC85
                U1kKdEZveC9Gb0JSVUdVVXFJV0ExYWsvRStwK295bkRZTUNNSFlNUjlhcWhxai9HeUZaT0VtVVlt
                RCt5UUtiMWZCLwprVUs3ayswN1lxUGR5SjN5algwUWdPcmhVQktPTk84NFpDNFZrYkZENlRKK3pN
                aWtkanlMSGRLZFE2Y29ZTVFECkx2OVNiYnFlY2wzWWJxMWh2NEppVjdIcy9zRmZvNTNOT0JQMDB4
                ZnNOay9NYmJEY2NQa2hBdzU4TmFKYVo2VDQKb2Vuamt0eXZaOVNZa2pwYUxTc0JtR3ZBRm1xdEI1
                eHpkRjFrT1FJREFRQUJvQUF3RFFZSktvWklodmNOQVFFTApCUUFEZ2dFQkFLaDlPTjVwOUU1b0pR
                dDkvRmZma3Z1dis0QkgwUVd6M0ErRU04WjBheExMZ0MyNVZIZ2dBaDVKCmhBZGhUWFEwN2VpZWZv
                MUxjazIxVXRHQU1vcU8wdnpzOWFSUUhleXZKZkJ6R0lSbzlwclBISFJKdEk3THB1VE8KR2JwRjRk
                Qks2djl3UW1vZFVZZlpmOUtDcUJyYmIxUUJCRysrdlpnVHpoRkhtMzQ2QkxzYU1LenZrUEIreFNS
                WgpEZTJGOGl0a3NDTFQwdnVKWTlEUWh2Y3lqZzk4ejdiTEU3ZkN0MDhRbjNEUUFEM1dqOUlDckFP
                dGYrNzhzeURkCktFZXJ5cXVpa0NVdlFtb0lxV0NOSCtIQTVVSDh2cG9DRlFwQkFDTXZMU2FRblp5
                WC9mNWY3Q2xiWWhOdHNVQ1IKTkdxelF5c3BReG1uUzRrRlcwaERhZWpOSzM1a29rRT0KLS0tLS1F
                TkQgQ0VSVElGSUNBVEUgUkVRVUVTVC0tLS0tCg==


            3) now a admin will create a csr manifest file as below:
                root@k8master:~# cat sam-csr.yaml
                apiVersion: certificates.k8s.io/v1
                kind: CertificateSigningRequest
                metadata:
                    name: sam_csr
                spec:
                    groups:
                    - system:authenticated
                    usages:
                    - client auth
                    request: LS0tLS1CRUdJTiBDRVJUSUZJQ0FURSBSRVFVRVNULS0tLS0KTUlJQ1V6Q0NBVHNDQVFBd0RqRU1NQW9HQTFVRUF3d0RjMkZ0TUlJQklqQU5CZ2txaGtpRzl3MEJBUUVGQUFPQwpBUThBTUlJQkNnS0NBUUVBdHdJOE5NaGxibFJRQm9XMVVUbGRMUkNNUm1HNWZPSlpDZ1Fjcy9GOERmUllBL2t0CnZPQk1sNk1rRjFDQWRsV1NieW5FWnZWUVo5TFVkYkwwbTdsVkRLM2oxNFloY04xVVhZTEMzWnBLZURPaC85U1kKdEZveC9Gb0JSVUdVVXFJV0ExYWsvRStwK295bkRZTUNNSFlNUjlhcWhxai9HeUZaT0VtVVltRCt5UUtiMWZCLwprVUs3ayswN1lxUGR5SjN5algwUWdPcmhVQktPTk84NFpDNFZrYkZENlRKK3pNaWtkanlMSGRLZFE2Y29ZTVFECkx2OVNiYnFlY2wzWWJxMWh2NEppVjdIcy9zRmZvNTNOT0JQMDB4ZnNOay9NYmJEY2NQa2hBdzU4TmFKYVo2VDQKb2Vuamt0eXZaOVNZa2pwYUxTc0JtR3ZBRm1xdEI1eHpkRjFrT1FJREFRQUJvQUF3RFFZSktvWklodmNOQVFFTApCUUFEZ2dFQkFLaDlPTjVwOUU1b0pRdDkvRmZma3Z1dis0QkgwUVd6M0ErRU04WjBheExMZ0MyNVZIZ2dBaDVKCmhBZGhUWFEwN2VpZWZvMUxjazIxVXRHQU1vcU8wdnpzOWFSUUhleXZKZkJ6R0lSbzlwclBISFJKdEk3THB1VE8KR2JwRjRkQks2djl3UW1vZFVZZlpmOUtDcUJyYmIxUUJCRysrdlpnVHpoRkhtMzQ2QkxzYU1LenZrUEIreFNSWgpEZTJGOGl0a3NDTFQwdnVKWTlEUWh2Y3lqZzk4ejdiTEU3ZkN0MDhRbjNEUUFEM1dqOUlDckFPdGYrNzhzeURkCktFZXJ5cXVpa0NVdlFtb0lxV0NOSCtIQTVVSDh2cG9DRlFwQkFDTXZMU2FRblp5WC9mNWY3Q2xiWWhOdHNVQ1IKTkdxelF5c3BReG1uUzRrRlcwaERhZWpOSzM1a29rRT0KLS0tLS1FTkQgQ0VSVElGSUNBVEUgUkVRVUVTVC0tLS0tCg==
                    signerName: kubernetes.io/kube-apiserver-client

            4) now we can create the csr
                root@k8master:~# kubectl create -f sam-csr.yaml
                certificatesigningrequest.certificates.k8s.io/sam_csr created

            5) now we can list the csr and check its created but pending now
                root@k8master:~# kubectl get csr
                NAME        AGE   SIGNERNAME                                    REQUESTOR                 REQUESTEDDURATION   CONDITION
                csr-2bvnb   95m   kubernetes.io/kube-apiserver-client-kubelet   system:bootstrap:nq0583   <none>              Approved,Issued
                csr-d2cpm   98m   kubernetes.io/kube-apiserver-client-kubelet   system:node:k8master      <none>              Approved,Issued
                csr-tbj6s   94m   kubernetes.io/kube-apiserver-client-kubelet   system:bootstrap:nq0583   <none>              Approved,Issued
                sam_csr     4s    kubernetes.io/kube-apiserver-client           kubernetes-admin          <none>              Pending

            6) now approve the csr and list again.
                root@k8master:~# kubectl certificate approve sam_csr
                certificatesigningrequest.certificates.k8s.io/sam_csr approved
                root@k8master:~# kubectl get csr
                NAME        AGE    SIGNERNAME                                    REQUESTOR                 REQUESTEDDURATION   CONDITION
                csr-2bvnb   101m   kubernetes.io/kube-apiserver-client-kubelet   system:bootstrap:nq0583   <none>              Approved,Issued
                csr-d2cpm   104m   kubernetes.io/kube-apiserver-client-kubelet   system:node:k8master      <none>              Approved,Issued
                csr-tbj6s   101m   kubernetes.io/kube-apiserver-client-kubelet   system:bootstrap:nq0583   <none>              Approved,Issued
                sam_csr     19s    kubernetes.io/kube-apiserver-client           kubernetes-admin          <none>              Approved,Issued

            7) Now we can extract the cert and share with user:
                root@k8master:~# kubectl get csr sam_csr -o yaml   (grab the certificate under status: certifiate)

                but this is base64 endoded format so decode this and share with user.

                root@k8master:~# echo "LS0tLS1CRUdJTiBDR............" | base64 -d

                -----BEGIN CERTIFICATE-----
                MIIC8zCCAdugAwIBAgIQUadw02NbR6tGVIg0DLXyWDANBgkqhkiG9w0BAQsFADAV
                MRMwEQYDVQQDEwprdWJlcm5ldGVzMB4XDTI1MDkyOTA5MDA0MloXDTI2MDkyOTA5
                MDA0MlowDjEMMAoGA1UEAxMDc2FtMIIBIjANBgkqhkiG9w0BAQEFAAOCAQ8AMIIB
                CgKCAQEAtwI8NMhlblRQBoW1UTldLRCMRmG5fOJZCgQcs/F8DfRYA/ktvOBMl6Mk
                F1CAdlWSbynEZvVQZ9LUdbL0m7lVDK3j14YhcN1UXYLC3ZpKeDOh/9SYtFox/FoB
                RUGUUqIWA1ak/E+p+oynDYMCMHYMR9aqhqj/GyFZOEmUYmD+yQKb1fB/kUK7k+07
                YqPdyJ3yjX0QgOrhUBKONO84ZC4VkbFD6TJ+zMikdjyLHdKdQ6coYMQDLv9Sbbqe
                cl3Ybq1hv4JiV7Hs/sFfo53NOBP00xfsNk/MbbDccPkhAw58NaJaZ6T4oenjktyv
                Z9SYkjpaLSsBmGvAFmqtB5xzdF1kOQIDAQABo0YwRDATBgNVHSUEDDAKBggrBgEF
                BQcDAjAMBgNVHRMBAf8EAjAAMB8GA1UdIwQYMBaAFDqM5XaK/mQQFZhycB8T9+Ez
                6sGZMA0GCSqGSIb3DQEBCwUAA4IBAQB4/x5c4v5LlY/oJxeQBqd8wxvzSCfAax6U
                5lYbDDfBrmxUqMB2jRdKRrvEzYAd4n6Dk1t6KdBRT2vl0qj+/T1Z0bxucKeLPYi1
                mN4IP8a9zq5kZKg/ilW9ZbULv1ZvVODHYLmsfIvdQRr5YMNiOq1pKjJTIBCV+fud
                wkQ/SfNHtdvWbGiwyVExyt4i8PSq+GATwwkywg4O0Toc+lOwsUZDBGxVL0QWSX1B
                j2FWJQQDJz5K2MHBbMqyfGHkqlRRA5OjoBcGXWt+PKn4QwDETVe3UTlTDCqsXMnN
                gSEA1ZuTjmFpBFys6TqPrvZcGamholhxJQIp9qORptab9X89xe4U
                -----END CERTIFICATE-----

            8) user can creates its own .kube/config file and authenticate himself to cluster but he has right role binding then only he can list or get the resources.

    --> Understanding kubeconfig file
            generally this file is used for kubectl command to authenicate and be default looks under $HOME/.kube/config or we can export the path with KUBECONFIG parameter and then run kubectl command.

            kubeconfig file is having 3 sections:
                - clusters
                - context
                - users

                ** clusters: is an array where we put the certifactes, IP of cluster, name of clusters (just for our reference in this file)
                ** Users: is an array where we put the user ceritifcate and name (give any name just for our reference in this file)
                ** context: where we define the cluster name, user name degined in above two parameter and we can name this context.

                root@k8master:~# cat .kube/config
                apiVersion: v1
                clusters:
                - cluster:
                    certificate-authority: <path_to_certificate_file>
                    certificate-authority-data: LS0tLS1CRUd.............. (in base64 encoded format)
                    server: https://10.0.0.4:6443
                name: kubernetes
                contexts:
                - context:
                    cluster: kubernetes
                    user: kubernetes-admin
                    namespace: <namespace_name> (if you want to switch automatically to a particular namespace)
                name: kubernetes-admin@kubernetes
                current-context: kubernetes-admin@kubernetes
                kind: Config
                preferences: {}
                users:
                - name: kubernetes-admin
                user:
                    client-certificate-data: LS0tLS1...........
                     client-key-data: LS0tLS1CRUdJTiBSU..........

            ** if we have multiple context in this config file then we can specify the default one with "current-context: kubernetes-admin@kubernetes"

            ** we can alos view the config with kubectl command:
                # kubectl config view

            ** if you need to change the current context then we can do:
                # kubectl config use-context <user_name>@<cluster_name>

            ** for more information, we can check the help for:
                # kubectl config -h

            ** if required, we can set the default namespace under contexts fields:
                namespace: <namespace_name>

26) Authorization:
        to provide the minimum level of access to developers or service accounts.
        to give access to read only cluster wide
        to restrict access to a particular namespace only

        below mechanism are supported:
        A) Node Authorizer:
            where kubelet reports the node or pods running on that node back to kube-apiserver so authentication method like certifactes must be part of system:node group.

        B) ABAC (Attribite based access control):
            where we can specify the list of actions for a group of user in JSON format and pass this to kube-apiserver for each user hence difficult to manage.
        
        C) RBAC (Role based access contro):
            Instead of directly specifying the list of actions, we can create a role with list of actions and then bind the same with users or group.
            It is easier way to manage the roles

        D) Webhook:
            When we outsource the access for user and group, where a kube-apiserver will give a api call to Open Policy Agent before authentocation and authorization.

        E) AlwaysAllow:
            By default, allows everything without any authorization checks.

        F) AlwaysDeny:
            Deny all requests.

        To specify all the modes or whichever is required, we need to specify in kube-apiserver manifest file:
            - --authorization-mode=Node,RBAC  (we can specify multiple modes putting comma in between and when a user tries to authorize, it wll be performed in sequence)

                (so here, request first goes to NODE, which is only for node authorizer so it will be denied, then it will go to RBAC and if user allows there then request will be allowed, it not then goes to next mode and if already allowed then checks are stopped and no more mode will be checked)


    RBAC in details:
    --> Role and RoleBinding are namespaced object and can be used in defined namespace scope.
        ** Create a ROLE object file: (or # kubectl create role <role_name> --verb=<allowed_actions> --resource=<allowed_resource)
        apiVersion: rbac.authorization.k8s.io/v1
        kind: Role
        metadata:
          name: app-team
        rules:
        - apiGroups: [""]  (for core api group, it will be blank, for class api, we need to put the api name)
          resources: ["pods"]   (resource name belongs to that api group)
          verbs: ["get", "list", "create"]  (list of actions belong to that resource)
          resourceNames: ["<pod_name"] (to restrict to only specific pod in that namespace)
        
        - apiGroups: [""] (we can add mulitple sections with resource and verbs under rules)

        ** Assocaite the user or group to that role using ROLEBINDING (or # kubectl create rolebinding <name> --role=<role_name> --user=<user_name>)
        apiVersion: rbac.authorization.k8s.io/v1
        kind: RoleBinding
        metadata:
          name: app-team-binding
        subjects:
        - kind: User
          name: developer-1      
          apiGroup: rbac.authorization.k8s.io
        roleRef:
          kind: Role
          name: app-team
          apiGroup: rbac.authorization.k8s.io

    --> Once created, as a user we can check the access whether a particular task or action can be performed or not:
        # kubectl auth can-i <action> <resource>

        root@k8master:~# kubectl auth can-i create deployments
        yes

        Being an admin also, we can check for other user like:
        root@k8master:~# kubectl auth can-i create deployments --as developer-1 (after --as <user_name>)
        no

        Or if in a particular namespace we need to verify then:
        root@k8master:~# kubectl auth can-i create deployments --as developer-1 --namespace app1 (after --namespace <namespace_name>)
        no

    --> CLUSTERROLE and CLUSTERROLEBINDING (these are clustered scope previleages)
        like: node-admin roles can view, add, delete nodes from cluster.
        storage-admin can view, create, delete PV/PVCs

        apiVersion: rbac.authorization.k8s.io/v1
        kind: ClusterRole
        metadata:
          name: node-admin
        rules:
        - apiGroups: [""]
          resources: ["nodes"]
          verbs: ["get", "list", "create", "delete"]

        piVersion: rbac.authorization.k8s.io/v1
        kind: ClusterRoleBinding
        metadata:
          name: node-admin-binding
        subjects:
        - kind: User
          name: cluser-admin     
          apiGroup: rbac.authorization.k8s.io
        roleRef:
          kind: ClusterRole
          name: node-admin
          apiGroup: rbac.authorization.k8s.io

        --> We can create a cluster role for a particular resource which are namespaced only and an associated user or group can access those across all namespaces, meaning cluster wide.

    --> SERVICE ACCOUNTS 
            An account which is used to intergrate the third party tools to get the data or authenticate for some purpose.
            like, SA for an monioring tools to call the metrics api to get the performance of cluster.

            we can simple create a service account with kubectl command, and we can bind/create the token for the SA which we can integrate in third party app to authenicate to the kubernetes cluster.
            root@k8master:~# kubectl create sa jenkins-sa (to create a SA)
            serviceaccount/jenkins-sa created

            root@k8master:~# kubectl get sa (to list a SA)
            NAME         SECRETS   AGE
            default      0         7h15m
            jenkins-sa   0         5s

            root@k8master:~# kubectl describe sa jenkins-sa
            Name:                jenkins-sa
            Namespace:           default
            Labels:              <none>
            Annotations:         <none>
            Image pull secrets:  <none>
            Mountable secrets:   <none>
            Tokens:              <none>
            Events:              <none>

            To create a token for a particular SA:
            option-1) Create a secret manually (static token  not recommended anymore)
            apiVersion: v1
            kind: Secret
            metadata:
            name: jenkins-sa-token
            annotations:
                kubernetes.io/service-account.name: jenkins-sa
            type: kubernetes.io/service-account-token

            then:
            root@k8master:~# kubectl create -f jenkins-sa-secret.yaml
            root@k8master:~# kubectl get secret
            NAME               TYPE                                  DATA   AGE
            jenkins-sa-token   kubernetes.io/service-account-token   3      5s

            root@k8master:~# kubectl describe sa jenkins-sa
            Name:                jenkins-sa
            Namespace:           default
            Labels:              <none>
            Annotations:         <none>
            Image pull secrets:  <none>
            Mountable secrets:   <none>
            Tokens:              jenkins-sa-token
            Events:              <none>

            option-2) Use the TokenRequest API (preferred, short-lived)
            # kubectl create token jenkins-sa
            You can then use this token in a kubeconfig or for CI/CD tools (like Jenkins).
            or to a curl command as Bearer token:
            # curl https://10.0.0.4:6443/api --header "Authorization: Bearer eyJhbGciOiJSUzI......."

            (if curl gives you a SSL Certificate error, so need to create a kube-config file for this user and pass the token)


            apiVersion: v1
            kind: Config
            clusters:
            - name: kubernetes
            cluster:
                certificate-authority: /etc/kubernetes/pki/ca.crt   # or full path to your ca.crt
                server: https://10.0.0.4:6443                       # replace with your API server
            contexts:
            - name: jenkins-sa@kubernetes
            context:
                cluster: kubernetes
                namespace: default
                user: jenkins-sa
            current-context: jenkins-sa@kubernetes
            users:
            - name: jenkins-sa
            user:
                token: TOKEN-HERE

            ** each namespace has a default service account and by default mounted inside in each pod until you set it false at this location - /var/run/secrets/kubernetes.io/serviceaccount/token
            we can sepcify the property under spec section:
                automountServiceAccountToken: false

            Now, in latest K8s, its uses the short live token which gets auto renewed.
            Mounts:
                /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-x6fwc (ro)
            Volumes:
            kube-api-access-x6fwc:
                Type:                    Projected (a volume that contains injected data from multiple sources)
                TokenExpirationSeconds:  3607
                ConfigMapName:           kube-root-ca.crt
                ConfigMapOptional:       <nil>
                DownwardAPI:             true

            If wanted to pod to use your SA then specify in this spec section:
                serviceAccountName: <SA_name>

    --> Image hosted in private registry:
            In pod definition file, what if the images is hosted in our private registry then we need to pass the credentials to access the same.
            So in order to pass the credentials in pod definition file, we create a secret as docker-registry type and pass the all required information in secret and then pass the registry server url, username and password. and while writing the pod definition file we can specify the secret as imagePullSecrets property.

            root@k8master:~# kubectl create secret docker-registry --help
            Create a new secret for use with Docker registries.

                    Dockercfg secrets are used to authenticate against Docker registries.

                    When using the Docker command line to push images, you can authenticate to a given registry by running:
                    '$ docker login DOCKER_REGISTRY_SERVER --username=DOCKER_USER --password=DOCKER_PASSWORD --email=DOCKER_EMAIL'.

            That produces a ~/.dockercfg file that is used by subsequent 'docker push' and 'docker pull' commands to authenticate
            to the registry. The email address is optional.

                    When creating applications, you may have a Docker registry that requires authentication.  In order for the
                    nodes to pull images on your behalf, they must have the credentials.  You can provide this information
                    by creating a dockercfg secret and attaching it to your service account.

            Examples:
            # If you do not already have a .dockercfg file, create a dockercfg secret directly
            kubectl create secret docker-registry my-secret --docker-server=DOCKER_REGISTRY_SERVER --docker-username=DOCKER_USER
            --docker-password=DOCKER_PASSWORD --docker-email=DOCKER_EMAIL

    --> Security Context inside the pod or container:
            Let say, we want to add a particular capabilities or run that container or pod with specific user like we did for docker, then we can specifies it at pod level or container level or both.
            at pod level:
                under spec sections:
                    securityContext:
                        runAsUser: <uid>

            at container level, then put it under container section:

            (we can check more capabilities in /usr/include/linux/capability.h which is a Linux kernel header file that defines the capabilities framework. We use it in Containers: Docker, Kubernetes, Podman drop/assign Linux capabilities to Pods/containers (instead of full root))
                under spec sections:
                    securityContext:
                        runAsUser: <uid>
                        capabilities:  (supports at conteiner level only)
                            add: ["CAP_CHOWN"]      # Change file ownership
                            add: ["NET_ADMIN"]      # lets the container configure networking.
                            drop: ["MKNOD"]         # it cannot create device nodes.

            to check the current, permitted, and effective capabilities
            # capsh --print

27) Network Policies:
            A NetworkPolicy in Kubernetes controls pod-to-pod traffic (and pod-to-external traffic) at the network layer (L3/L4).
            By default, all pods can talk to all other pods. NetworkPolicies allow you to restrict traffic.

            They work only if your clusters CNI plugin supports them (Calico, Cilium, Weave Net, etc.  but not Flannel).

            Key Concepts
                Pod Selector  Which pods the policy applies to.
                Ingress  Incoming traffic to the pod.
                Egress  Outgoing traffic from the pod.
                Namespace Selector  Restrict by namespace.
                IPBlock  Restrict by IP ranges.

            manifest file explanations:
            - apiVersion: networking.k8s.io/v1
            - kind: NetworkPolicy
            - metadata:
                name: <name_of_policy>
                namespace: <where_this_policy_to_be_applied>
            - spec:
                podSelector:             (on which pod it will be applicable, make sure pod is in same namespace which is defined under metadata)
                    matchLabels:
                        <key>: <value>
                policyTypes:
                - Ingress
                ingress:
                - from:
                  - podSelector:       (from which pod it will be allowed, make sure pod is in same namespace which is defined under metadata)
                      matchLabels:
                        <key>: <value>
                  - namespaeSelector:   (here namespaceSeletor is put as dedicated or seprate property so all pods from this namespace are allowed)
                      matchLabels:      (if no - for namespaceSeletor then it will consider under podSelector so in that case, pod defined in that must be present in this \\)
                        <key>: <value>  (namespace then only access will be allowed)
                  - ipBlock:
                      cidr: <192.168.0.0/16>   (allowd the range of IPs which are allowed to the pod where this policy is imposed)
                  ports:
                  - protocol: <TCP/UDP>
                    port: <port_number>

                - Egress
                egress:
                - to:
                  - ipBlock:
                      cidr: <192.168.0.0/16>
                  ports:
                  - protocol: <TCP/UDP>
                    port: <port_number>

28) Service:
        When we need to access the applicaiton hosted in pod, we need to expose this as service. We have already learned Service Types.
        Each service gets its own IP which is different from pod and when a service is created, it spanned across all network and can be reached via it IP from any pod and any namespaces. 
        When a service is getting created, it got the IP from pre-defined range of IPs. To assign the IP address to a service, it is done by Kube-Porxy and it created a forwarding rule via IP Tables to Pod IP and updated the table across all nodes.

        Kube-proxy is doing it by creating and deleting these rules, by default it uses a IPTables mode, but it can set as USERSPACE (service listens on port and forstward the connections), IPVs (forward rules),. If you wish you change it then set --proxy-mode [<>] to desired mode.

        can be checked inside:
        root@k8master:~/CKA/labs# k exec -it kube-proxy-rlkxq -n kube-system -- cat /var/lib/kube-proxy/config.conf | grep -i mode
        detectLocalMode: ""
        mode: ""     (if empty, the default IPTables is used)

        IP range are set by Kube-apiserver and can be checked in manifest file:
        root@k8master:~/CKA/labs# cat /etc/kubernetes/manifests/kube-apiserver.yaml | grep -i service-cluster-ip-range
        - --service-cluster-ip-range=10.96.0.0/12

        (and POD CIDR are set when we initialize our cluster --> kubeadm init --pod-network-cidr=10.244.0.0/16)
        (for node IP, we can check the used CNI plugin logs, in our case, we check weave)
        root@k8master:~/CKA/labs# k logs weave-net-kdzt9 -c weave -n kube-system | grep -i ipalloc
        1:6784 ipalloc-init:consensus=3 ipalloc-range:10.32.0.0/12 metrics-addr:0.0.0.0:6782 name:a2:cb:d3:a2:69:db nickname:k8node1 no-dns:true no-masq-local:true port:6783]

        ---> Range for Service IP and Pod CIDR should not be overlapped.

        Check the IPTables forward rule:
        root@k8master:~/CKA/labs# iptables -L -t nat | grep -i <service_name>  (will list the rule where it forwards the service IP to POD IP)

        example:
        root@k8master:~/CKA/labs# kubectl get pods -o wide
        NAME        READY   STATUS    RESTARTS      AGE   IP          NODE      NOMINATED NODE   READINESS GATES
        def-nginx   1/1     Running   1 (82m ago)   22h   10.36.0.3   k8node1   <none>           <none>
        root@k8master:~/CKA/labs#
        root@k8master:~/CKA/labs# kubectl get svc
        NAME            TYPE        CLUSTER-IP      EXTERNAL-IP   PORT(S)   AGE
        def-nginx-svc   ClusterIP   10.100.215.56   <none>        80/TCP    22h
        kubernetes      ClusterIP   10.96.0.1       <none>        443/TCP   3d12h
        root@k8master:~/CKA/labs#
        root@k8master:~/CKA/labs# iptables -L -t nat | grep -i def-nginx-svc
        KUBE-MARK-MASQ  all  --  10.36.0.3            anywhere             /* default/def-nginx-svc */
        DNAT       tcp  --  anywhere             anywhere             /* default/def-nginx-svc */ tcp to:10.36.0.3:80
        KUBE-SVC-HPDRBN24IAJTBMI6  tcp  --  anywhere             10.100.215.56        /* default/def-nginx-svc cluster IP */ tcp dpt:http
        KUBE-SEP-ASQLJ5AUARHHDWUH  all  --  anywhere             anywhere             /* default/def-nginx-svc -> 10.36.0.3:80 */

        --> to check the logs, when a service is created, it makes an entry in /var/log/kube-proxy.log


    --> CORE DNS:
            Kuberenets set up its inbuilt DNS with cluster setup, and run as core-dns pod into master nodes.
            When a new service is created, DNS record is updated and now instead of service IP, we can access it with name as well.
            If in same namespace, then we can access it with servie name only.
            If in a different namespace, then we need to append the namespace name at the end of service name.
            It can further divided into subdomain called svc and then root domain, cluster.local

            so finally, we have full name (FQDN) as 
            <service_name>.<namespace_name>.svc.cluser.local

        --> By default, record are updated or created for PODS as well, if you want then you can enable or disable.
            It gets recorded as, let say pod ip is 10.36.0.1 then name will be 10-36-0-1 but can be accessed only with fqdn
            and FQDN as - <IP_in_dash_format>.<namespace_name>.pod.cluster.local

        --> Coredns pods main config file is mapped as configmap in kube-system namespace.
                root@k8master:~/CKA/labs# k describe configmap coredns -n kube-system
                Name:         coredns
                Namespace:    kube-system
                Labels:       <none>
                Annotations:  <none>

                Data
                ====
                Corefile:
                ----
                .:53 {
                    errors
                    health {
                    lameduck 5s
                    }
                    ready
                    kubernetes cluster.local in-addr.arpa ip6.arpa {
                    pods insecure
                    fallthrough in-addr.arpa ip6.arpa
                    ttl 30
                    }
                    prometheus :9153
                    forward . /etc/resolv.conf {
                    max_concurrent 1000
                    }
                    cache 30
                    loop
                    reload
                    loadbalance
                }


                BinaryData
                ====

            --> In this file, entry "pods insecure" takes care of the pods dns entry but we have 3 mode for this:
                | Mode            | Description                                                                                                                       |
                | --------------- | --------------------------------------------------------------------------------------------------------------------------------- |
                | `pods insecure` | Registers pod names using their IPs, even if not verified by the API. Allows reverse DNS lookups (PTR). Old and less secure mode. |
                | `pods verified` | Registers pods only after verifying against the Kubernetes API (recommended).                                                     |
                | `pods disabled` | Disables per-pod DNS records completely (no A or PTR for pods).                                                                   |

                ** when we say pods known to the API, were referring to Pods that are officially registered and managed by the Kubernetes API server
                ** So, any Pod you create through the Kubernetes API (using kubectl, a YAML manifest, or a controller like a Deployment or DaemonSet) is known to the API.
                ** If you just start a container manually using docker run or inject an IP record manually, CoreDNS in pods verified mode will ignore it  because it doesnt exist in the Kubernetes API.
                ** Static containers started by docker run

                ** To recreate or modify Pod DNS records
                # kubectl -n kube-system edit configmap coredns
                # Replace insecure with verified (recommended) OR Whatever the requirement
                # kubectl -n kube-system rollout restart deployment coredns

        --> coredns runs as a replicaset and exposed as a service on IP 10.96.0.10 which can be use as DNS server for each pod:
            root@k8master:~/CKA/labs# k get svc -n kube-system
            NAME       TYPE        CLUSTER-IP   EXTERNAL-IP   PORT(S)                  AGE
            kube-dns   ClusterIP   10.96.0.10   <none>        53/UDP,53/TCP,9153/TCP   3d14h

            This IP we can use in each pod as primary DNS server, so that pod or service can be resolved by each other with FQDN. Where as CORE DNS pod, in /etc/resolv.conf file, it has been set to same nameserver IP as in master node so that if pod or any app try to reach internet they can use this nameserver.

        --> root@k8master:~/CKA/labs# k exec -it def-nginx -- cat /etc/resolv.conf
            search default.svc.cluster.local svc.cluster.local cluster.local pdjeoyiylobuxaxlt2mnakeuib.gx.internal.cloudapp.net
            nameserver 10.96.0.10

        --> root@k8master:~/CKA/labs# k exec -it def-pod-4 -n dev -- cat /etc/resolv.conf
            search dev.svc.cluster.local svc.cluster.local cluster.local pdjeoyiylobuxaxlt2mnakeuib.gx.internal.cloudapp.net
            nameserver 10.96.0.10

29) Ingress
        Helps to redirect the external traffic to internal multiple services and implement the SSL Security as well.
        It's a kind of Layer 7 LB but inbuilt in Kubernetes cluster itself.
        But to make it avialable externally, we still need to expose it.
        So, we can manage, authentcation for application, SSL load, url based routing, all can be done at this layer.

        Without ingress: (manual)
            We can deploy nginx, HAProxy or traefik reverse proxy inside the kubernetes.
            Configure the URL Routes, SSL certificate

        With ingress: (as a kuberenetes object)
            Kubernetes also follow the same logic and any one of solution from above.
            This resource or solution is called ingress controller and set of rule or configuration is called ingress resources.
            These resouces can be created using definition files.
            Ingress Controller is not the default setup, we need to setup it at time of cluster installation.

        Ingress contoller Solution:
            
            | Ingress Controller             | Maintained by Kubernetes Project | Notes                                                |
            | ------------------------------ | -------------------------------- | ---------------------------------------------------- |
            | **Ingress-NGINX**              |  Yes                            | Official and widely used                             |
            | **GCE Ingress**                |  Yes (for GKE only)             | Maintained for Google Cloud                          |
            | **Traefik**                    |  No                             | External project                                     |
            | **HAProxy Ingress**            |  No                             | External, production-grade                           |
            | **NGINX Plus (by F5)**         |  No                             | Commercial, different project                        |
            | **Istio / Contour / Kong**     |  No                             | Service mesh or API gateway style ingress            |
            | **AWS ALB Ingress Controller** |  No                             | Maintained by AWS                                    |
            | **NGINX Ingress (F5 version)** |  No                             | Maintained separately from Kubernetes ingress-nginx |

             Repo:
             https://github.com/kubernetes/ingress-nginx  (official)
            But we will use this:
            https://raw.githubusercontent.com/kubernetes/ingress-nginx/main/deploy/static/provider/baremetal/deploy.yaml
            
        Deploy the solution:
            1) We can deploy the nginx-ingress-controller as a deployment.
            2) # special built image of nginx to be used as an ingress controller in K8s: registry.k8s.io/ingress-nginx/controller:v1.11.2
            3) there are few configuration of nginx which also need to pass to pod, so in order to pass, create a configmap and mount the same inside the pod definition.
            Like: err-log-path, keep-alive, ssl-protocols, timeout data etc.
            (if you are not sure, as of now these values, then create a empty configmap and later we can set this easily)
            4) set the two environment variable which have a value as POD Name and Namespace it is deployed to (better to deploy in dedicated namespace). These env variable is required by nginx service to read the configuration data wihtin the POD
            5) Set the port for ingress controller (80 and 443)
            6) create a  service type of NodePort with label selector to link the service.
            7) create a role, cluster role and bind the same with service account
            
            We are ready to deploy our ingress-controller, now create the ingress resources:
            8) Ingress rules are set of rule and configuration applied on the ingress controller where we can define the to forward all incoming traffic to a single applicaiton or route traffic to different applications (services), based on path (URL). for an example- /, /support, /help, /demo
            - here / is the default domain or app, let say mywebapp, then
            - mywebapp/support then route to support app(service)
            - mywebapp/help then route to help app(service)
            - mywebapp/demo then route to demo app(service)

            Lets check out the demo now.
            - deploy a single app without ingress and feel the problems that for each service we need a dedicated node port
            - deploy the ingress and then deploy the same app and access it with ingress service not the app service post creatig the host rules.

    
    Ingress Limitation:
        - It support only Host matching, Path matching with HTTP only.
        - It did not support for TCP/UDP routing, traffic spliting/weighting, header manipulation, authentication, rate limiting, redirects, session affinity, but we can pass those as annotations to controllers to different ingress controller such as for nginx its different and which is not supported by others. So these configurations are very specfic to underlying contollers.

        ** To overcome this, K8s official project Gateway API can help.
            Gateway API support INgress, Load Balancing, Service Mesh APIs.
            To separate the conroll for entire Infra Yaml (like we had in Ingress Nginx), it creates the 3 class:
                a) Gateway Class
                    Where we specify the What infrastructure provider will be used, like nginx ingress controller, or other LoabBalancer.
                b) Gateway
                    Cluster Operator which are instances of the gateway class
                c) HTTPRoutes
                    Application Developers manages this for application as per need.
            ** Gateway API already imlemented in Nginx Gateway Fabric (GA)
                
30) Kuberenets CRDs (Custom Resource Definitions)
        Its a way to extend Kubernetes with your own new resource types  beyond the built-in ones like Pod, Service, Deployment, etc.
        You can define a new type like Database, or Backup but in order to work this actual, we need a Controller who actuall does the job so we can write our own conrtoller in GO lang and create a new object in kuberenetes. So, these both resources are custom like custom object and custom controller and to make these avilable as api we need to create a CRD resources where we need to defing the schemas, version, stroed versions, kind, group and few more.

        Refer: # k get crd certificaterequests.cert-manager.io -o yaml
        -->
        apiVersion: apiextensions.k8s.io/v1
        kind: CustomResourceDefinition
        metadata:
            name: <name_of_crd> 
        spec:
            scope: Namespaced (or non spaced)
            group: <api_class_group>  # like mycrd/v1 as apiVersion
            names:
              kind: <type> (like Pod, Deployment)
              singular: <singlular_name_of_type>
              plural: <plural_name_of_types>
              shortNames:
                <any_short_name_kind_type> (like we have cm for configmaps)
            versions:
              - name: <version_of_crd> (like v1, v2)
                served: true
                storage: true 
                schema:
                  openAPI3Schema: (defined what all fields are supported under spec and the type)
                  type: object
                  properties:
                    spec:
                      type: object
                      properties:
                         description: (it could be any, description is just an example)
                            type: string
                         number:      (it could be any, number is just an example)
                            type: integer
                            minimum: 1
                            maximum: <n>

        ** Basically 2 steps are reqired to create your own object, one is CRDs file and controller then we can map the controller setup as image and run as pod inside the cluster and create these 2 resources.

        ** But it can be binded together as Operator Framework. And then we create the operator or install it, it internally create the CRD resources and controller as deployment.

        ** we can refer https://operatorhub.io/ for more info, where we have already pre-defined operators, we can simply downlaod, install and use it.





31) Liveness & Readiness Probes (Explained + Demo)
        Liveness Probe  Checks if the container is still alive.
                         If it fails  Kubelet kills and restarts the container.

                          Used to auto-heal stuck apps (e.g., deadlock or unresponsive).

        Readiness Probe  Checks if the container is ready to serve traffic.
                          If it fails  Pod is removed from Service endpoints but not restarted.

                          Used when app startup takes time (e.g., DB connections).

        Demo - Liveness probe failed:
        Events:
        Type     Reason     Age                   From               Message
        ----     ------     ----                  ----               -------
        Normal   Scheduled  17m                   default-scheduler  Successfully assigned default/probe-demo to k8node2
        Normal   Pulled     17m                   kubelet            Successfully pulled image "busybox" in 1.522s (1.522s including waiting). Image size: 2223686 bytes.
        Normal   Pulled     16m                   kubelet            Successfully pulled image "busybox" in 339ms (339ms including waiting). Image size: 2223686 bytes.
        Normal   Created    15m (x3 over 17m)     kubelet            Created container: demo-app
        Normal   Started    15m (x3 over 17m)     kubelet            Started container demo-app
        Normal   Pulled     15m                   kubelet            Successfully pulled image "busybox" in 493ms (493ms including waiting). Image size: 2223686 bytes.
        Warning  Unhealthy  14m (x9 over 17m)     kubelet            Liveness probe failed: cat: can't open '/tmp/healthy': No such file or directory
        Normal   Killing    14m (x3 over 17m)     kubelet            Container demo-app failed liveness probe, will be restarted
        Normal   Pulling    14m (x4 over 17m)     kubelet            Pulling image "busybox"
        Normal   Pulled     12m                   kubelet            Successfully pulled image "busybox" in 388ms (388ms including waiting). Image size: 2223686 bytes.
        Warning  BackOff    2m34s (x29 over 10m)  kubelet            Back-off restarting failed container demo-app in pod probe-demo_default                        (b90befbd-bc1c-4f4c-bae0-11dbeb7e1b7


        Demo - Readiness probe failed:
        Events:
        Type     Reason     Age                 From               Message
        ----     ------     ----                ----               -------
        Normal   Scheduled  2m11s               default-scheduler  Successfully assigned default/probe-real-demo to k8node1
        Normal   Pulling    2m11s               kubelet            Pulling image "busybox"
        Normal   Pulled     2m11s               kubelet            Successfully pulled image "busybox" in 359ms (359ms including waiting). Image size: 2223686 bytes.
        Normal   Created    2m11s               kubelet            Created container: init-wait
        Normal   Started    2m11s               kubelet            Started container init-wait
        Normal   Killing    86s                 kubelet            Container demo-app failed liveness probe, will be restarted
        Normal   Pulled     56s (x2 over 2m5s)  kubelet            Container image "python:3.9-slim" already present on machine
        Normal   Created    56s (x2 over 2m5s)  kubelet            Created container: demo-app
        Normal   Started    56s (x2 over 2m5s)  kubelet            Started container demo-app
        Warning  Unhealthy  16s (x4 over 91s)   kubelet            Liveness probe failed: Get "http://10.36.0.2:8080/": dial tcp 10.36.0.2:8080: connect: connection refused
        Warning  Unhealthy  16s (x10 over 91s)  kubelet            Readiness probe failed: Get "http://10.36.0.2:8080/": dial tcp 10.36.0.2:8080: connect: connection refused





            
        













        



                        















